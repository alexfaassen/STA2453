{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156.44160040416168\n"
     ]
    }
   ],
   "source": [
    "## Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "## Script imports\n",
    "import simuFlares\n",
    "from STL_IF import STLIF\n",
    "import detectFlare\n",
    "from sigma_clip import sigma_clip\n",
    "## Simulation status\n",
    "from IPython.display import clear_output\n",
    "\n",
    "## Setup\n",
    "# Load Data\n",
    "pdcsap = pd.read_csv(\"../0.Data/031381302.csv\", index_col = 'time').loc[:, [\"pdcsap_flux\"]].dropna()\n",
    "# Calm interval\n",
    "pdcsap = pdcsap.query(\"1442 <= index <= 1449\")\n",
    "inds = np.arange(pdcsap.shape[0])\n",
    "\n",
    "## Flare parameters\n",
    "num_flares = 5\n",
    "# Base half-peak timescale: larger values => all flares last longer (relative to their amplitudes)\n",
    "t_half = 4.32/120 #2.5  # e.g. 10 minutes (2-min cadence)\n",
    "# Flare ampltiude (Pareto) parameters\n",
    "xm = pdcsap['pdcsap_flux'].mean() * 0.02        # Scale (~ x_min): Baseline amplitude (values will rarely be smaller than this)\n",
    "alpha = 2                                       # Shape: smaller => heavier tail = more large flares\n",
    "offset = 0                                      # Offset amplitudes (shift)\n",
    "upper = pdcsap['pdcsap_flux'].mean() * 0.1      # Amplitude cap\n",
    "print(upper)\n",
    "# xm = 10         # Scale (~ x_min): Baseline amplitude (values will rarely be smaller than this)\n",
    "# alpha = 1       # Shape: smaller => heavier tail = more large flares.\n",
    "# offset = 30     # Offset amplitudes (shift)\n",
    "# upper = 100     # Amplitude cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n",
      "After 100 runs:\n",
      "STLIF:\n",
      "  Avg Precision: 0.980\n",
      "  Avg Recall:    0.830\n",
      "  Avg F1 Score:  0.886\n",
      "3-3sigma:\n",
      "  Avg Precision: 0.850\n",
      "  Avg Recall:    0.292\n",
      "  Avg F1 Score:  0.420\n"
     ]
    }
   ],
   "source": [
    "## Isolation Forest parameters\n",
    "contamination = 0.001 # Expected proportion of anomalies\n",
    "n_estimators = 100 # Number of trees\n",
    "sample_size = 256 # Number of samples used to train each tree\n",
    "\n",
    "## Simulate\n",
    "n = 100 # Number of simulations\n",
    "stlif_metrics = []\n",
    "sigma_metrics = []\n",
    "\n",
    "for i in range(n):\n",
    "    ## Simulation status\n",
    "    clear_output(wait=True)\n",
    "    print(i+1)\n",
    "\n",
    "    ## Simulate flares\n",
    "    flare_lightcurve, flare_times = simuFlares.kepler_flare(\n",
    "        inds,                           # time array\n",
    "        t_half,                         # base half-peak width\n",
    "        num_flares,                     # number of flares\n",
    "        flux_dist=simuFlares.rpareto,   # amplitude distribution\n",
    "        xm=xm, alpha=alpha, offset=offset, upper=upper\n",
    "    )\n",
    "    # Inject flares\n",
    "    data = pdcsap.copy()\n",
    "    data[\"pdcsap_flux\"] += flare_lightcurve\n",
    "\n",
    "    ## Run model: STLIF\n",
    "    data = STLIF(data, contamination=contamination, n_estimators=n_estimators, sample_size=sample_size)\n",
    "\n",
    "    # Calculate metrics\n",
    "    prec, rec, f1 = detectFlare.event_level_scores(real_flares=flare_times, y_pred=data[\"anomaly\"].values)\n",
    "    stlif_metrics.append((prec, rec, f1))\n",
    "\n",
    "    ## Run model: STLSigmaClip\n",
    "    # Note: Uses detrended series from STLIF output.\n",
    "    anomalies = sigma_clip(data['resid'], sigma=3.0, consecutive_pts=3).ravel()\n",
    "\n",
    "    # Calculate metrics\n",
    "    prec, rec, f1 = detectFlare.event_level_scores(real_flares=flare_times, y_pred=anomalies)\n",
    "    sigma_metrics.append((prec, rec, f1))\n",
    "\n",
    "## Compute average metrics\n",
    "avg_prec, avg_rec, avg_f1 = np.array(stlif_metrics).mean(axis=0)\n",
    "\n",
    "# Print results\n",
    "print(f\"After {n} runs:\")\n",
    "print(\"STLIF:\")\n",
    "print(f\"  Avg Precision: {avg_prec:.3f}\")\n",
    "print(f\"  Avg Recall:    {avg_rec:.3f}\")\n",
    "print(f\"  Avg F1 Score:  {avg_f1:.3f}\")\n",
    "\n",
    "## Compute average metrics\n",
    "avg_prec, avg_rec, avg_f1 = np.array(sigma_metrics).mean(axis=0)\n",
    "\n",
    "print(\"3-3sigma:\")\n",
    "print(f\"  Avg Precision: {avg_prec:.3f}\")\n",
    "print(f\"  Avg Recall:    {avg_rec:.3f}\")\n",
    "print(f\"  Avg F1 Score:  {avg_f1:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combination: 1 (contamination=0.001, n_est=100, m_samp=256)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m<__array_function__ internals>:179\u001b[0m, in \u001b[0;36mpartition\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\numpy\\core\\fromnumeric.py:663\u001b[0m, in \u001b[0;36m_partition_dispatcher\u001b[1;34m(a, kth, axis, kind, order)\u001b[0m\n\u001b[0;32m    603\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    604\u001b[0m \u001b[38;5;124;03m    Reverse or permute the axes of an array; returns the modified array.\u001b[39;00m\n\u001b[0;32m    605\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    658\u001b[0m \n\u001b[0;32m    659\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    660\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapfunc(a, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranspose\u001b[39m\u001b[38;5;124m'\u001b[39m, axes)\n\u001b[1;32m--> 663\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_partition_dispatcher\u001b[39m(a, kth, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, kind\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    664\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (a,)\n\u001b[0;32m    667\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_partition_dispatcher)\n\u001b[0;32m    668\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpartition\u001b[39m(a, kth, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, kind\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mintroselect\u001b[39m\u001b[38;5;124m'\u001b[39m, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'statsmodels.tsa._stl.STL._rwts'\n",
      "Traceback (most recent call last):\n",
      "  File \"<__array_function__ internals>\", line 179, in partition\n",
      "  File \"C:\\Users\\alexd\\AppData\\Roaming\\Python\\Python311\\site-packages\\numpy\\core\\fromnumeric.py\", line 663, in _partition_dispatcher\n",
      "    def _partition_dispatcher(a, kth, axis=None, kind=None, order=None):\n",
      "    \n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "## Isolation Forest parameters\n",
    "# Expected proportion of anomalies\n",
    "contamination_values = [0.001, 0.005, 0.01, 0.02]\n",
    "# Number of trees\n",
    "n_estimators_values = [100, 200]\n",
    "# Number of samples used to train each tree\n",
    "max_samples_values = [256, \"auto\"]\n",
    "\n",
    "## Simulate\n",
    "n_runs = 10 # Number of simulations\n",
    "results = []\n",
    "k = 1 # Counter\n",
    "\n",
    "# Create a small param grid\n",
    "param_grid = []\n",
    "for c in contamination_values:\n",
    "    for ne in n_estimators_values:\n",
    "        for ms in max_samples_values:\n",
    "            param_grid.append((c, ne, ms))\n",
    "\n",
    "for (contamination, n_est, m_samp) in param_grid:\n",
    "    ## Simulation status\n",
    "    clear_output(wait=True)\n",
    "    print(\"Combination: \", k, \" (contamination=\", contamination, \", n_est=\", n_est, \", m_samp=\", m_samp, \")\", sep=\"\")\n",
    "    k += 1\n",
    "\n",
    "    ## Setup\n",
    "    run_metrics = []\n",
    "    \n",
    "    for run_i in range(n_runs):\n",
    "        ## Simulate flares\n",
    "        flare_lightcurve, flare_times = simuFlares.kepler_flare(\n",
    "            inds,                           # time array\n",
    "            t_half,                         # base half-peak width\n",
    "            num_flares,                     # number of flares\n",
    "            flux_dist=simuFlares.rpareto,   # amplitude distribution\n",
    "            xm=xm, alpha=alpha, offset=offset, upper=upper\n",
    "        )\n",
    "        # Inject flares\n",
    "        data = pdcsap.copy()\n",
    "        data[\"pdcsap_flux\"] += flare_lightcurve\n",
    "\n",
    "        ## Run model: STLIF\n",
    "        data = STLIF(data, contamination=contamination, n_estimators=n_est, sample_size=m_samp)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        prec, rec, f1 = detectFlare.event_level_scores(real_flares=flare_times, y_pred=data[\"anomaly\"].values)\n",
    "        run_metrics.append((prec, rec, f1))\n",
    "    \n",
    "    # Average performance over n_runs\n",
    "    avg_prf = np.mean(run_metrics, axis=0)\n",
    "    result_dict = {\n",
    "        \"contamination\": contamination,\n",
    "        \"n_estimators\": n_est,\n",
    "        \"max_samples\": m_samp,\n",
    "        \"avg_precision\": avg_prf[0],\n",
    "        \"avg_recall\":    avg_prf[1],\n",
    "        \"avg_f1_score\":  avg_prf[2],\n",
    "    }\n",
    "    results.append(result_dict)\n",
    "\n",
    "# Sort results by F1\n",
    "results.sort(key=lambda x: x[\"avg_f1_score\"], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Hyperparam Combos (by F1):\n",
      "{'contamination': 0.001, 'n_estimators': 100, 'max_samples': 256, 'avg_precision': 0.9833333333333332, 'avg_recall': 0.8400000000000001, 'avg_f1_score': 0.8992424242424242}\n",
      "{'contamination': 0.005, 'n_estimators': 100, 'max_samples': 256, 'avg_precision': 0.8672619047619048, 'avg_recall': 0.9399999999999998, 'avg_f1_score': 0.8925796425796427}\n"
     ]
    }
   ],
   "source": [
    "# Print top results\n",
    "print(\"Top 5 Hyperparam Combos (by F1):\")\n",
    "for row in results[:5]:\n",
    "    print(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
