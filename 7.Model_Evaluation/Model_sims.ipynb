{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156.44160040416168\n"
     ]
    }
   ],
   "source": [
    "## Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "## Script imports\n",
    "import simuFlares\n",
    "from STL_IF import STLIF\n",
    "import detectFlare\n",
    "from sigma_clip import sigma_clip\n",
    "## Simulation status\n",
    "from IPython.display import clear_output\n",
    "\n",
    "## Setup\n",
    "# Load Data\n",
    "pdcsap = pd.read_csv(\"../0.Data/031381302.csv\", index_col = 'time').loc[:, [\"pdcsap_flux\"]].dropna()\n",
    "# Calm interval\n",
    "pdcsap = pdcsap.query(\"1442 <= index <= 1449\")\n",
    "inds = np.arange(pdcsap.shape[0])\n",
    "\n",
    "## Flare parameters\n",
    "num_flares = 5\n",
    "# Base half-peak timescale: larger values => all flares last longer (relative to their amplitudes)\n",
    "t_half = 4.32/120 #2.5  # e.g. 10 minutes (2-min cadence)\n",
    "# Flare ampltiude (Pareto) parameters\n",
    "xm = pdcsap['pdcsap_flux'].mean() * 0.02        # Scale (~ x_min): Baseline amplitude (values will rarely be smaller than this)\n",
    "alpha = 2                                       # Shape: smaller => heavier tail = more large flares\n",
    "offset = 0                                      # Offset amplitudes (shift)\n",
    "upper = pdcsap['pdcsap_flux'].mean() * 0.1      # Amplitude cap\n",
    "print(upper)\n",
    "# xm = 10         # Scale (~ x_min): Baseline amplitude (values will rarely be smaller than this)\n",
    "# alpha = 1       # Shape: smaller => heavier tail = more large flares.\n",
    "# offset = 30     # Offset amplitudes (shift)\n",
    "# upper = 100     # Amplitude cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n",
      "After 100 runs:\n",
      "STLIF:\n",
      "  Avg Precision: 0.980\n",
      "  Avg Recall:    0.830\n",
      "  Avg F1 Score:  0.886\n",
      "3-3sigma:\n",
      "  Avg Precision: 0.850\n",
      "  Avg Recall:    0.292\n",
      "  Avg F1 Score:  0.420\n"
     ]
    }
   ],
   "source": [
    "## Isolation Forest parameters\n",
    "contamination = 0.001 # Expected proportion of anomalies\n",
    "n_estimators = 100 # Number of trees\n",
    "sample_size = 256 # Number of samples used to train each tree\n",
    "\n",
    "## Simulate\n",
    "n = 100 # Number of simulations\n",
    "stlif_metrics = []\n",
    "sigma_metrics = []\n",
    "\n",
    "for i in range(n):\n",
    "    ## Simulation status\n",
    "    clear_output(wait=True)\n",
    "    print(i+1)\n",
    "\n",
    "    ## Simulate flares\n",
    "    flare_lightcurve, flare_times = simuFlares.kepler_flare(\n",
    "        inds,                           # time array\n",
    "        t_half,                         # base half-peak width\n",
    "        num_flares,                     # number of flares\n",
    "        flux_dist=simuFlares.rpareto,   # amplitude distribution\n",
    "        xm=xm, alpha=alpha, offset=offset, upper=upper\n",
    "    )\n",
    "    # Inject flares\n",
    "    data = pdcsap.copy()\n",
    "    data[\"pdcsap_flux\"] += flare_lightcurve\n",
    "\n",
    "    ## Run model: STLIF\n",
    "    data = STLIF(data, contamination=contamination, n_estimators=n_estimators, sample_size=sample_size)\n",
    "\n",
    "    # Calculate metrics\n",
    "    prec, rec, f1 = detectFlare.event_level_scores(real_flares=flare_times, y_pred=data[\"anomaly\"].values)\n",
    "    stlif_metrics.append((prec, rec, f1))\n",
    "\n",
    "    ## Run model: STLSigmaClip\n",
    "    # Note: Uses detrended series from STLIF output.\n",
    "    anomalies = sigma_clip(data['resid'], sigma=3.0, consecutive_pts=3).ravel()\n",
    "\n",
    "    # Calculate metrics\n",
    "    prec, rec, f1 = detectFlare.event_level_scores(real_flares=flare_times, y_pred=anomalies)\n",
    "    sigma_metrics.append((prec, rec, f1))\n",
    "\n",
    "## Compute average metrics\n",
    "avg_prec, avg_rec, avg_f1 = np.array(stlif_metrics).mean(axis=0)\n",
    "\n",
    "# Print results\n",
    "print(f\"After {n} runs:\")\n",
    "print(\"STLIF:\")\n",
    "print(f\"  Avg Precision: {avg_prec:.3f}\")\n",
    "print(f\"  Avg Recall:    {avg_rec:.3f}\")\n",
    "print(f\"  Avg F1 Score:  {avg_f1:.3f}\")\n",
    "\n",
    "## Compute average metrics\n",
    "avg_prec, avg_rec, avg_f1 = np.array(sigma_metrics).mean(axis=0)\n",
    "\n",
    "print(\"3-3sigma:\")\n",
    "print(f\"  Avg Precision: {avg_prec:.3f}\")\n",
    "print(f\"  Avg Recall:    {avg_rec:.3f}\")\n",
    "print(f\"  Avg F1 Score:  {avg_f1:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combination: 5/5 (contamination=0.0012, n_est=200, m_samp=auto)\n"
     ]
    }
   ],
   "source": [
    "## Isolation Forest parameters\n",
    "# Expected proportion of anomalies\n",
    "contamination_values = [0.001, 0.0009, 0.0008, 0.0011, 0.0012]\n",
    "# Number of trees\n",
    "n_estimators_values = [200]\n",
    "# Number of samples used to train each tree\n",
    "max_samples_values = [\"auto\"]\n",
    "\n",
    "## Simulate\n",
    "n_runs = 25#10 # Number of simulations\n",
    "results = []\n",
    "# Counter\n",
    "k = 1\n",
    "import itertools\n",
    "total_k = len(list(itertools.product(contamination_values, n_estimators_values, max_samples_values))) # Total parameter combinations\n",
    "\n",
    "# Create a small param grid\n",
    "param_grid = []\n",
    "for c in contamination_values:\n",
    "    for ne in n_estimators_values:\n",
    "        for ms in max_samples_values:\n",
    "            param_grid.append((c, ne, ms))\n",
    "\n",
    "for (contamination, n_est, m_samp) in param_grid:\n",
    "    ## Simulation status\n",
    "    clear_output(wait=True)\n",
    "    print(\"Combination: \", k, \"/\", total_k, \" (contamination=\", contamination, \", n_est=\", n_est, \", m_samp=\", m_samp, \")\", sep=\"\")\n",
    "    k += 1\n",
    "\n",
    "    ## Setup\n",
    "    run_metrics = []\n",
    "    \n",
    "    for run_i in range(n_runs):\n",
    "        ## Simulate flares\n",
    "        flare_lightcurve, flare_times = simuFlares.kepler_flare(\n",
    "            inds,                           # time array\n",
    "            t_half,                         # base half-peak width\n",
    "            num_flares,                     # number of flares\n",
    "            flux_dist=simuFlares.rpareto,   # amplitude distribution\n",
    "            xm=xm, alpha=alpha, offset=offset, upper=upper\n",
    "        )\n",
    "        # Inject flares\n",
    "        data = pdcsap.copy()\n",
    "        data[\"pdcsap_flux\"] += flare_lightcurve\n",
    "\n",
    "        ## Run model: STLIF\n",
    "        data = STLIF(data, contamination=contamination, n_estimators=n_est, sample_size=m_samp)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        prec, rec, f1 = detectFlare.event_level_scores(real_flares=flare_times, y_pred=data[\"anomaly\"].values)\n",
    "        run_metrics.append((prec, rec, f1))\n",
    "    \n",
    "    # Average performance over n_runs\n",
    "    avg_prf = np.mean(run_metrics, axis=0)\n",
    "    result_dict = {\n",
    "        \"contamination\": contamination,\n",
    "        \"n_estimators\": n_est,\n",
    "        \"max_samples\": m_samp,\n",
    "        \"avg_precision\": avg_prf[0],\n",
    "        \"avg_recall\":    avg_prf[1],\n",
    "        \"avg_f1_score\":  avg_prf[2],\n",
    "    }\n",
    "    results.append(result_dict)\n",
    "\n",
    "# Sort results by F1\n",
    "results.sort(key=lambda x: x[\"avg_f1_score\"], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Hyperparam Combos (by F1):\n",
      "{'contamination': 0.001, 'n_estimators': 200, 'max_samples': 'auto', 'avg_precision': 1.0, 'avg_recall': 0.96, 'avg_f1_score': 0.9777777777777779}\n",
      "{'contamination': 0.0011, 'n_estimators': 200, 'max_samples': 'auto', 'avg_precision': 1.0, 'avg_recall': 0.9400000000000001, 'avg_f1_score': 0.9638888888888889}\n",
      "{'contamination': 0.0012, 'n_estimators': 200, 'max_samples': 'auto', 'avg_precision': 0.9833333333333334, 'avg_recall': 0.9399999999999998, 'avg_f1_score': 0.9575757575757577}\n",
      "{'contamination': 0.0009, 'n_estimators': 200, 'max_samples': 'auto', 'avg_precision': 1.0, 'avg_recall': 0.9199999999999999, 'avg_f1_score': 0.9555555555555557}\n",
      "{'contamination': 0.0008, 'n_estimators': 200, 'max_samples': 'auto', 'avg_precision': 1.0, 'avg_recall': 0.7999999999999999, 'avg_f1_score': 0.8888888888888891}\n"
     ]
    }
   ],
   "source": [
    "# Print top results\n",
    "print(\"Top 5 Hyperparam Combos (by F1):\")\n",
    "for row in results[:5]:\n",
    "    print(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
